{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic - Machine Learning from Disaster\n",
    "\n",
    "* Author: Lamin Bojang\n",
    "* Date created: 2023-06-02\n",
    "* Date modified: 2023-06-10\n",
    "* Description: Titanic data analysis and visualization\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:\n",
    "- [x] Import libraries\n",
    "- [x] Load data\n",
    "- [x] Data cleaning\n",
    "- [x] Data exploration\n",
    "- [x] Data visualization\n",
    "- [x]check for missing values\n",
    "- [x] Data imputation\n",
    "- [x] Drop unnecessary columns\n",
    "- [x] Convert categorical to numerical\n",
    "- [ ] Feature scaling\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Dictionary**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Variable | Definition                             | Key                                       |\n",
    "|----------|----------------------------------------|-------------------------------------------|\n",
    "| survival | Survival                               | 0 = No, 1 = Yes                           |\n",
    "| pclass   | Ticket class                           | 1 = 1st, 2 = 2nd, 3 = 3rd                 |\n",
    "| sex      | Sex                                    |                                           |\n",
    "| Age      | Age in years                           |                                           |\n",
    "| sibsp    | # of siblings / spouses aboard Titanic |                                           |\n",
    "| parch    | # of parents / children aboard Titanic |                                           |\n",
    "| ticket   | Ticket number                          |                                           |\n",
    "| fare     | Passenger fare                         |                                           |\n",
    "| cabin    | Cabin number                           |                                           |\n",
    "| embarked | Port of Embarkation                    | C = Cherbourg, Q = Queenstown, S = Southampton |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install missingno\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import missingno as msno\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "# Set the background color globally\n",
    "plt.rcParams['axes.facecolor'] = 'darkgray'\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set_style('darkgrid')\n",
    "\n",
    "palette = sns.color_palette(\"coolwarm\", 7)\n",
    "\n",
    "#markdown display in jupyter notebook output\n",
    "from IPython.core.display import Markdown as md\n",
    "\n",
    "#Enable jupyter notebook cells to display multiple outputs in one cell\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "#hide warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "\n",
    "def generate_dataframe_summary(dataframe):\n",
    "    \"\"\"\n",
    "    Generates a summary of the dataframe.\n",
    "    \"\"\"\n",
    "    summary = pd.DataFrame({\n",
    "        'features': dataframe.columns,\n",
    "        'dataType': dataframe.dtypes,\n",
    "        'unqValCount': dataframe.nunique(),\n",
    "        'nullsCount': dataframe.isna().sum()\n",
    "    })\n",
    "\n",
    "    summary['nullsPct'] = (summary['nullsCount'] / dataframe.shape[0]).round(4) * 100\n",
    "    summary['Unique values(All if < 10)'] = summary['features'].apply(lambda col: dataframe[col].unique()[:10])\n",
    "\n",
    "    return summary.sort_values(by=[\"dataType\", 'unqValCount'], ascending=False).reset_index(drop=True)\n",
    "\n",
    "\n",
    "def encode_features(dataframe, features, method='onehot'):\n",
    "    \"\"\"\n",
    "    Encodes specified features using either one-hot or label encoding.\n",
    "\n",
    "    Parameters:\n",
    "    features: A list of feature names to encode.\n",
    "    method: The encoding method to use. Either 'onehot' or 'label'. Default is 'onehot'.\n",
    "    \"\"\"\n",
    "    encoded_df = dataframe.copy()\n",
    "\n",
    "    if method == 'onehot':\n",
    "        encoder = OneHotEncoder()\n",
    "        for feature in features:\n",
    "            encoded_feature = pd.DataFrame(encoder.fit_transform(encoded_df[[feature]]).toarray())\n",
    "            encoded_feature.columns = encoder.get_feature_names_out([feature])\n",
    "            encoded_df = pd.concat([encoded_df, encoded_feature], axis=1).drop([feature], axis=1)\n",
    "    elif method == 'label':\n",
    "        encoder = LabelEncoder()\n",
    "        for feature in features:\n",
    "            encoded_df[feature] = encoder.fit_transform(encoded_df[feature])\n",
    "    else:\n",
    "        raise ValueError(\"Method must be either 'onehot' or 'label'.\")\n",
    "\n",
    "    return encoded_df\n",
    "\n",
    "def correlation_matrix(df):\n",
    "    corr = df.corr()\n",
    "    figure = plt.figure(figsize=(8, 5))\n",
    "    sns.heatmap(corr, xticklabels=corr.columns.values, yticklabels=corr.columns.values, annot=True, cmap='coolwarm')\n",
    "    plt.title('Correlation Heatmap', fontsize=8)\n",
    "    plt.show()\n",
    "    \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import CSV to a pandas DataFrame\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('train.csv')\n",
    "df_test = pd.read_csv('test.csv')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"### First Three Rows of the Training dataframe\")\n",
    "df_train.head(3)\n",
    "md(\"### Random Sampling from the Training Data Frame\")\n",
    "df_train.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"### First Three Rows of the Test dataframe\")\n",
    "df_test.head(3)\n",
    "md(\"### Random Sampling from the Test Data Frame\")\n",
    "df_test.sample(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataframe_summary(df_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataframe_summary(df_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Data Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist plot of numeric columns in the Training Data Frame\n",
    "df_train.hist(figsize=(20, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hist plot of numeric columns in the Training Data Frame\n",
    "df_test.hist(figsize=(20, 10));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot of categorical columns\n",
    "fig, axes = plt.subplots(3,2 , figsize=(15, 10))\n",
    "fig.set_facecolor('darkgray')\n",
    "\n",
    "sns.countplot(data=df_train, x= 'Sex',  hue='Survived', ax=axes[0][0])\n",
    "sns.countplot(data=df_train, x= 'Embarked',  hue='Survived', ax=axes[0][1])\n",
    "sns.countplot(data=df_train, x= 'Pclass',  hue='Survived', ax=axes[1][0])\n",
    "sns.countplot(data=df_train, x= 'SibSp',  hue='Survived', ax=axes[1][1])\n",
    "sns.countplot(data=df_train, x= 'Parch',  hue='Survived', ax=axes[2][0])\n",
    "# Add a placeholder plot in the last subplot\n",
    "axes[2][1].axis('off')\n",
    "\n",
    "plt.tight_layout();\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 2, figsize=(15, 10))\n",
    "fig.suptitle('Survival Rate by Sex, Embarked, Pclass, SibSp, Parch', fontsize=20)\n",
    "sns.countplot(data=df_train, x='Sex', hue='Survived', ax=axes[0][0])\n",
    "sns.countplot(data=df_train, x='Embarked', hue='Survived', ax=axes[0][1])\n",
    "sns.countplot(data=df_train, x='Pclass', hue='Survived', ax=axes[1][0])\n",
    "sns.countplot(data=df_train, x='SibSp', hue='Survived', ax=axes[1][1])\n",
    "sns.countplot(data=df_train, x='Parch', hue='Survived', ax=axes[2][0])\n",
    "\n",
    "# Add a placeholder plot in the last subplot\n",
    "axes[2][1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data  Cleaning"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"### Train Data Summary\")\n",
    "generate_dataframe_summary(df_train)\n",
    "\n",
    "md(\"### Test Data Summary\")\n",
    "generate_dataframe_summary(df_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_train)\n",
    "plt.title('Training Data Missing Values', fontsize=25)\n",
    "plt.show();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "msno.matrix(df_test)\n",
    "plt.title('Test Data Missing Values', fontsize=25)\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> From looking at the Data  we can see we have to take the following Initial steps:\n",
    "- Remove the `Cabin`\tfeature as it is  missing almost 78% of the rows and imputing wont be accurate it has  too much variataion in it to affect our  prediction.\n",
    "- Remove `Name` and `PassengerId` columns since  they are unique to every single customer and have no predictive value  for our analysis\n",
    "- `Age` has about 20% missing in both train and test dataset. The distribution is slightly skewed right so simple imputation will not give us th best outcome. So will use KNN imputer with an N of 5 and then check the standard Deviation and distribution\n",
    "- checking for outlier in the  dataset  won't be necessary due  to high variability in the dataset as indicated by the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"### Age Column Statistics Before Imputation\")\n",
    "df_train['Age'].describe().round(2).to_frame().T\n",
    "\n",
    "md(\"### Age Column nulls count before imputation\")\n",
    "df_train['Age'].isnull().value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot age distribution\n",
    "df_train['Age'].hist(bins=20)\n",
    "plt.title('Age Distribution Before Imputation', fontsize=10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Drop Features/columns not required for analysis\n",
    "The following variable will be dropped from the dataframe:\n",
    "- `PassengerId`: unique identifier for passengers no discernable pattern.\n",
    "- `Name`: Not needed for analysis as it has \n",
    "- `ticket`: This feature has 681 unique values amd duplicate values in some fields\n",
    "- `cabin`: Contains too many missing  values to be imputed, not to mention all missingness seems to be in class 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = df_train.copy()\n",
    "\n",
    "#drop 'Cabin', 'Ticket', 'Name', 'PassengerId' columns reset index\n",
    "traindf.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "md(\"**Train Data null values before imputation**\")\n",
    "pd.DataFrame(traindf.isnull().sum(), columns=[\"nullsCount\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = df_test.copy()\n",
    "\n",
    "#drop 'Cabin', 'Ticket', 'Name', 'PassengerId' columns reset index\n",
    "testdf.drop(['Cabin', 'Ticket', 'Name', 'PassengerId'], axis=1, inplace=True)\n",
    "\n",
    "md(\"**Test Data null values before imputation**\")\n",
    "pd.DataFrame(testdf.isnull().sum(), columns=[\"nullsCount\"]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill missing embarked in the Train data null values with mode \n",
    "traindf['Embarked'].fillna(traindf['Embarked'].mode()[0], inplace=True)\n",
    "print(f\"Train  Data Embarked feature has {traindf.Embarked.isna().sum()} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill missing fare value in the test data with the mean\n",
    "testdf['Fare'].fillna(testdf['Fare'].mean(), inplace=True)\n",
    "print(f\"Test data Fare feature has {testdf.Fare.isna().sum()} null values\")\n",
    "\n",
    "#fill missing Embarked value in the test data with the mean\n",
    "testdf['Embarked'].fillna(testdf['Embarked'].mode()[0], inplace=True)\n",
    "print(f\"Test data Embarked feature has {testdf.Fare.isna().sum()} null values\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use knn imputer to fill missing values in the Age column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "traindf['Age'] = imputer.fit_transform(traindf[['Age']])\n",
    "testdf['Age'] = imputer.fit_transform(testdf[['Age']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"#### Age Column Statistics After Imputation\")\n",
    "traindf['Age'].describe().round(2).to_frame().T\n",
    "\n",
    "print(f\"Age feature has {traindf.Age.isna().sum()} null values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot age distribution\n",
    "\n",
    "traindf['Age'].hist(bins=20)\n",
    "plt.title('Age Distribution After Imputation', fontsize=10);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"#### Train Data Summary\")\n",
    "generate_dataframe_summary(traindf)\n",
    "\n",
    "md(\"#### Test Data Summary\")\n",
    "generate_dataframe_summary(testdf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf = encode_features(traindf, ['Embarked', 'Sex'], method='onehot') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testdf = encode_features(testdf, ['Embarked', 'Sex'], method='onehot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataframe_summary(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_dataframe_summary(testdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(traindf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_matrix(testdf)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check imbalance in the survived column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md(\"**Count of people who survived and who did not**\")\n",
    "traindf['Survived'].value_counts().to_frame()\n",
    "\n",
    "md(\"**Percentage of people who survived and who did not**\")\n",
    "np.round(traindf['Survived'].value_counts(normalize=True) * 100, 2).to_frame()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the count of survived and not survived\n",
    "sns.countplot(x='Survived', data=traindf);"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> **From the data  and Graph above we  can see the data is  not imbalanced  to the point of requiring mitigation steps.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md('**Average survival rate by feature**')\n",
    "\n",
    "traindf.groupby('Survived').mean().round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "traindf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Data Modeling\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training steps\n",
    "In this  step we will use a machine learning classifier algorithm to train our data byt taking the following steps:\n",
    "\n",
    "- Split the  training data into input(x) and target(Y) variables\n",
    "- Do a test run with multiple algorithms and  inspect the scores to see which one give the  best  Accuracy.\n",
    "- Run a feature reduction algorithm against  multiple classifiers to  get the optimum amount of feature to streamline our model.\n",
    "- Reduce  the training to the chosen features only\n",
    "- Scale the Training Data\n",
    "- Rerun again against the  clssifiers that had the best scores and compare the scores again to see if we had any improvement in accuracy.\n",
    "- "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_classifiers(df):\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X = df.drop(\"Survived\", axis=1)\n",
    "    y = df[\"Survived\"]\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    # Define the list of classifiers\n",
    "    classifiers = [\n",
    "        LogisticRegression(),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        XGBClassifier(),\n",
    "        SVC()\n",
    "    ]\n",
    "\n",
    "    # Dictionary to store the performance metrics\n",
    "    performance = {}\n",
    "\n",
    "    # Iterate over the classifiers\n",
    "    for classifier in classifiers:\n",
    "        # Train the classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        y_pred = classifier.predict(X_val)\n",
    "\n",
    "        # Evaluate performance\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        # Store the performance metrics\n",
    "        performance[classifier.__class__.__name__] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }\n",
    "\n",
    "    # Convert the performance metrics to a DataFrame\n",
    "    performance_df = pd.DataFrame(performance)\n",
    "\n",
    "    return performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# def evaluate_classifiers(df):\n",
    "#     # Split the data into features (X) and target variable (y)\n",
    "#     X = df.drop(\"Survived\", axis=1)\n",
    "#     y = df[\"Survived\"]\n",
    "\n",
    "#     # Split the data into train and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_val = scaler.transform(X_val)\n",
    "    \n",
    "#     # Define the list of classifiers\n",
    "#     classifiers = [\n",
    "#         LogisticRegression(),\n",
    "#         DecisionTreeClassifier(),\n",
    "#         RandomForestClassifier(),\n",
    "#         XGBClassifier(),\n",
    "#         SVC()\n",
    "#     ]\n",
    "\n",
    "#     # Dictionary to store the performance metrics\n",
    "#     performance = {}\n",
    "\n",
    "#     # Iterate over the classifiers\n",
    "#     for classifier in classifiers:\n",
    "#         # Train the classifier\n",
    "#         classifier.fit(X_train, y_train)\n",
    "\n",
    "#         # Make predictions on the validation set\n",
    "#         y_pred = classifier.predict(X_val)\n",
    "\n",
    "#         # Evaluate performance\n",
    "#         accuracy = accuracy_score(y_val, y_pred)\n",
    "#         precision = precision_score(y_val, y_pred)\n",
    "#         recall = recall_score(y_val, y_pred)\n",
    "#         f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "#         # Store the performance metrics\n",
    "#         performance[classifier.__class__.__name__] = {\n",
    "#             'Accuracy': accuracy,\n",
    "#             'Precision': precision,\n",
    "#             'Recall': recall,\n",
    "#             'F1-Score': f1\n",
    "#         }\n",
    "\n",
    "#     # Convert the performance metrics to a DataFrame\n",
    "#     performance_df = pd.DataFrame(performance)\n",
    "\n",
    "#     return performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from xgboost import XGBClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# def evaluate_classifiers(df):\n",
    "#     # Split the data into features (X) and target variable (y)\n",
    "#     X = df.drop(\"Survived\", axis=1)\n",
    "#     y = df[\"Survived\"]\n",
    "\n",
    "#     # Split the data into train and validation sets\n",
    "#     X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "#     scaler = StandardScaler()\n",
    "#     X_train = scaler.fit_transform(X_train)\n",
    "#     X_val = scaler.transform(X_val)\n",
    "    \n",
    "#     # Define the list of classifiers\n",
    "#     classifiers = [\n",
    "#         LogisticRegression(),\n",
    "#         DecisionTreeClassifier(),\n",
    "#         RandomForestClassifier(),\n",
    "#         XGBClassifier(),\n",
    "#         SVC()\n",
    "#     ]\n",
    "\n",
    "#     # Dictionary to store the performance metrics\n",
    "#     performance = {}\n",
    "\n",
    "#     # Iterate over the classifiers\n",
    "#     for classifier in classifiers:\n",
    "#         # Train the classifier\n",
    "#         classifier.fit(X_train, y_train)\n",
    "\n",
    "#         # Make predictions on the validation set\n",
    "#         y_pred = classifier.predict(X_val)\n",
    "\n",
    "#         # Evaluate performance\n",
    "#         accuracy = accuracy_score(y_val, y_pred)\n",
    "#         precision = precision_score(y_val, y_pred)\n",
    "#         recall = recall_score(y_val, y_pred)\n",
    "#         f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "#         # Store the performance metrics\n",
    "#         performance[classifier.__class__.__name__] = {\n",
    "#             'Accuracy': accuracy,\n",
    "#             'Precision': precision,\n",
    "#             'Recall': recall,\n",
    "#             'F1-Score': f1\n",
    "#         }\n",
    "\n",
    "#     # Convert the performance metrics to a DataFrame\n",
    "#     performance_df = pd.DataFrame(performance)\n",
    "\n",
    "#     return performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import pandas as pd\n",
    "\n",
    "def evaluate_classifiers(df):\n",
    "    # Split the data into features (X) and target variable (y)\n",
    "    X = df.drop(\"Survived\", axis=1)\n",
    "    y = df[\"Survived\"]\n",
    "\n",
    "    # Split the data into train and validation sets\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_val = scaler.transform(X_val)\n",
    "    \n",
    "    # Define the list of classifiers\n",
    "    classifiers = [\n",
    "        LogisticRegression(),\n",
    "        DecisionTreeClassifier(),\n",
    "        RandomForestClassifier(),\n",
    "        XGBClassifier(),\n",
    "        SVC()\n",
    "    ]\n",
    "\n",
    "    # Dictionary to store the performance metrics\n",
    "    performance = {}\n",
    "\n",
    "    # Iterate over the classifiers\n",
    "    for classifier in classifiers:\n",
    "        # Train the classifier\n",
    "        classifier.fit(X_train, y_train)\n",
    "\n",
    "        # Make predictions on the validation set\n",
    "        y_pred = classifier.predict(X_val)\n",
    "\n",
    "        # Evaluate performance\n",
    "        accuracy = accuracy_score(y_val, y_pred)\n",
    "        precision = precision_score(y_val, y_pred)\n",
    "        recall = recall_score(y_val, y_pred)\n",
    "        f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "        # Store the performance metrics\n",
    "        performance[classifier.__class__.__name__] = {\n",
    "            'Accuracy': accuracy,\n",
    "            'Precision': precision,\n",
    "            'Recall': recall,\n",
    "            'F1-Score': f1\n",
    "        }\n",
    "\n",
    "    # Convert the performance metrics to a DataFrame\n",
    "    performance_df = pd.DataFrame(performance)\n",
    "\n",
    "    return performance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_comparisons = evaluate_classifiers(traindf)\n",
    "display(cls_comparisons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Recursive Feature Elimination with Cross Validation\n",
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X = traindf.drop(\"Survived\", axis=1)\n",
    "y = traindf[\"Survived\"]\n",
    "\n",
    "# Split the data into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Define the classifier\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "# Define the RFECV\n",
    "rfecv = RFECV(estimator=classifier, step=1, cv=5, scoring='accuracy', n_jobs=-1, verbose=1)\n",
    "\n",
    "# Fit the RFECV\n",
    "rfecv.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot the accuracy score vs number of features\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.title('RFECV for RandomForestClassifier', fontsize=18, fontweight='bold', pad=20)\n",
    "plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n",
    "plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)    \n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'], color='#303F9F', linewidth=3)\n",
    "\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the optimal number of features\n",
    "\n",
    "print(f\"Optimal number of features : {rfecv.n_features_}\")\n",
    "\n",
    "# show the features rankings\n",
    "feature_importances = pd.DataFrame(rfecv.ranking_, index=X.columns, columns=['Rank']).sort_values(by='Rank', ascending=True)\n",
    "feature_importances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "from  sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "# Perform RFECV for each classifier\n",
    "classifiers = [\n",
    "    LogisticRegression(),\n",
    "    DecisionTreeClassifier(),\n",
    "    RandomForestClassifier(),\n",
    "    XGBClassifier(),\n",
    "    \n",
    "]\n",
    "\n",
    "\n",
    "for classifier in classifiers:\n",
    "    # Perform RFECV\n",
    "    selector = RFECV(estimator=classifier, cv=StratifiedKFold(5), scoring='accuracy')\n",
    "    selector.fit(X, y)\n",
    "\n",
    "    # Get the scores, feature rankings, and selected features\n",
    "    Classifier = classifier.__class__.__name__\n",
    "    # Print the results\n",
    "    md(f\"**Classifier: {Classifier}**\")\n",
    "    # print(f\"Scores: {scores}\")\n",
    "    # print(f\"Feature Rankings: {rankings}\")\n",
    "    # print(f\"Selected Features: {features}\")\n",
    "    # print(\"\\n\")\n",
    "    md(f\"**Optimal number of features : {rfecv.n_features_}**\")\n",
    "\n",
    "    # show the features rankings\n",
    "    feature_importances = pd.DataFrame(rfecv.ranking_, index=X.columns, columns=['Rank']).sort_values(by='Rank', ascending=True)\n",
    "    display(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the  low scoring  feature Embarked_S\tEmbarked_C\tEmbarked_Q\t\n",
    "\n",
    "train = traindf.copy()\n",
    "test = testdf.copy()\n",
    "train = train.drop(['Embarked_S', 'Embarked_C', 'Embarked_Q'], axis=1)\n",
    "test = test.drop(['Embarked_S', 'Embarked_C', 'Embarked_Q'], axis=1)\n",
    "train.head()\n",
    "testdf.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compare the performance of the classifiers on the reduced dataset\n",
    "cls_comparisons_Reduced = evaluate_classifiers(train)\n",
    "\n",
    "cls_comparisons_Reduced\n",
    "cls_comparisons\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#hyperparameter tuning using RandomizedSearchCV for XGBClassifier\n",
    "\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "X = train.drop(\"Survived\", axis=1)\n",
    "y = train[\"Survived\"]\n",
    "\n",
    "# Split the data into features (X) and target variable (y)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_val = scaler.transform(X_val)\n",
    "\n",
    "# Define the classifier\n",
    "classifier = XGBClassifier()\n",
    "\n",
    "# Define the hyperparameter configuration space\n",
    "n_estimators = [int(x) for x in np.linspace(start = 200, stop = 2000, num = 10)]\n",
    "max_depth = [int(x) for x in np.linspace(10, 110, num = 11)]\n",
    "learning_rate = [0.01, 0.05, 0.1, 0.2, 0.3]\n",
    "subsample = [0.5, 0.7, 0.9, 1.0]\n",
    "colsample_bytree = [0.5, 0.7, 0.9, 1.0]\n",
    "\n",
    "hyperparameter_grid = {\n",
    "    'n_estimators': n_estimators,\n",
    "    'max_depth': max_depth,\n",
    "    'learning_rate': learning_rate,\n",
    "    'subsample': subsample,\n",
    "    'colsample_bytree': colsample_bytree\n",
    "}\n",
    "\n",
    "# Define the RandomizedSearchCV object\n",
    "random_cv = RandomizedSearchCV(estimator=classifier,\n",
    "                                param_distributions=hyperparameter_grid,\n",
    "                                cv=5, n_iter=50,\n",
    "                                scoring = 'accuracy',n_jobs = -1,\n",
    "                                verbose = 1,\n",
    "                                return_train_score = True,\n",
    "                                random_state=42)\n",
    "\n",
    "# Fit on the training data and get the best model\n",
    "random_cv.fit(X_train, y_train)\n",
    "\n",
    "# Get the best estimator\n",
    "best_estimator = random_cv.best_estimator_\n",
    "\n",
    "# Make predictions using the best model\n",
    "y_pred = best_estimator.predict(X_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Get the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "print(f\"Accuracy: {accuracy}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Best Estimator: {best_estimator}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming clf is your fitted RandomizedSearchCV object\n",
    "# And XGBClassifier() without any arguments gives the default parameters\n",
    "\n",
    "best_params = random_cv.best_estimator_.get_params()\n",
    "default_params = XGBClassifier().get_params()\n",
    "\n",
    "# Find parameters in best_params that are not in default_params\n",
    "xb_params = {key: best_params[key] for key in best_params if best_params[key] != default_params[key]}\n",
    "\n",
    "print(xb_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgbc = XGBClassifier(**xb_params)\n",
    "xgbc.fit(X_train, y_train)\n",
    "y_val_pred = xgbc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.Series(y_val_pred).value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the accuracy score\n",
    "accuracy = accuracy_score(y_val, y_val_pred)\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = train.drop(\"Survived\", axis=1)\n",
    "y_train = train[\"Survived\"]\n",
    "\n",
    "X_test = test.copy()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X)\n",
    "\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "# Define the classifier\n",
    "classifier = XGBClassifier(**xb_params)\n",
    "\n",
    "# Fit the classifier\n",
    "classifier.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions using the best model\n",
    "ytest_pred = classifier.predict(X_test)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytest_pred value  counts\n",
    "pd.Series(ytest_pred).value_counts().to_frame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submission file\n",
    "submission = pd.DataFrame({'PassengerId': df_test.PassengerId, 'Survived': ytest_pred})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#export submission file to csv\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export model to a file\n",
    "import joblib\n",
    "joblib.dump(classifier, 'titanic_xgb_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### hidden section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame({'PassengerId': test['PassengerId'], 'Survived': Y_pred})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LogisticRegression\n",
    "# from sklearn.tree import DecisionTreeClassifier\n",
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.svm import SVC\n",
    "# from xgboost import XGBClassifier\n",
    "\n",
    "\n",
    "# from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "# import pandas as pd\n",
    "\n",
    "# # Split the data into features (X) and target variable (y)\n",
    "# X = traindf.drop(\"Survived\", axis=1)\n",
    "# y = traindf[\"Survived\"]\n",
    "\n",
    "# # Split the data into train and validation sets\n",
    "# X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Define the list of classifiers\n",
    "# classifiers = [\n",
    "#     LogisticRegression(),\n",
    "#     DecisionTreeClassifier(),\n",
    "#     RandomForestClassifier(),\n",
    "#     XGBClassifier(),\n",
    "#     SVC()\n",
    "# ]\n",
    "\n",
    "# # Dictionary to store the performance metrics\n",
    "# performance = {}\n",
    "\n",
    "# # Iterate over the classifiers\n",
    "# for classifier in classifiers:\n",
    "#     # Train the classifier\n",
    "#     classifier.fit(X_train, y_train)\n",
    "\n",
    "#     # Make predictions on the validation set\n",
    "#     y_pred = classifier.predict(X_val)\n",
    "\n",
    "#     # Evaluate performance\n",
    "#     accuracy = accuracy_score(y_val, y_pred)\n",
    "#     precision = precision_score(y_val, y_pred)\n",
    "#     recall = recall_score(y_val, y_pred)\n",
    "#     f1 = f1_score(y_val, y_pred)\n",
    "\n",
    "#     # Store the performance metrics\n",
    "#     performance[classifier.__class__.__name__] = {\n",
    "#         'Accuracy': accuracy,\n",
    "#         'Precision': precision,\n",
    "#         'Recall': recall,\n",
    "#         'F1-Score': f1\n",
    "#     }\n",
    "\n",
    "# # Print the performance metrics\n",
    "# for classifier, metrics in performance.items():\n",
    "#     md(f\"### {classifier}:\")\n",
    "#     for metric, value in metrics.items():\n",
    "#         md(f\"#### {metric}: {value.round(2)}\")\n",
    "#     print()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "polyglot_notebook": {
   "kernelInfo": {
    "defaultKernelName": "csharp",
    "items": [
     {
      "aliases": [],
      "name": "csharp"
     }
    ]
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
